{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96st5LKAVx3J",
        "outputId": "c1f4d708-001e-4ec0-a80f-204aae60d7a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Extracting bAbI...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3434102984.py:44: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(DATA_ROOT)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1000 Test: 1000\n",
            "Example answer: bathroom\n",
            "Vocab: 25 Answers: 6\n",
            "Epoch 01/8 | avg_loss=1.8048 | test_acc=0.170\n",
            "Epoch 08/8 | avg_loss=1.3714 | test_acc=0.414\n",
            "Final test accuracy: 0.414\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Vanilla bAbI (Task 1) + Transformer Encoder (NO constraints)\n",
        "# Colab-ready: download + parse bAbI, train, evaluate\n",
        "# ============================================================\n",
        "\n",
        "import os, re, tarfile, random, math\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import urllib.request\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -------------------------\n",
        "# Download bAbI\n",
        "# -------------------------\n",
        "DATA_ROOT = Path(\"./data_babi\")\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "url = \"https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\"\n",
        "tgz_path = DATA_ROOT / \"babi_tasks_1-20_v1-2.tar.gz\"\n",
        "extract_dir = DATA_ROOT\n",
        "\n",
        "if not tgz_path.exists():\n",
        "    print(\"Downloading bAbI...\")\n",
        "    urllib.request.urlretrieve(url, tgz_path)\n",
        "\n",
        "\n",
        "print(\"Extracting bAbI...\")\n",
        "with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
        "    tar.extractall(DATA_ROOT)\n",
        "\n",
        "# -------------------------\n",
        "# bAbI parser (Task 1 default)\n",
        "# -------------------------\n",
        "def tokenize(text: str):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
        "    return [t for t in text.split() if t.strip()]\n",
        "\n",
        "def load_babi_qa(path: Path, max_story_sentences=50):\n",
        "    samples = []\n",
        "    story = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            idx_str, rest = line.split(\" \", 1)\n",
        "            idx = int(idx_str)\n",
        "            if idx == 1:\n",
        "                story = []\n",
        "            if \"\\t\" in rest:\n",
        "                q, a, supporting = rest.split(\"\\t\")\n",
        "                story_tokens = []\n",
        "                for sent in story[-max_story_sentences:]:\n",
        "                    story_tokens += tokenize(sent) + [\"<ssep>\"]\n",
        "                q_tokens = tokenize(q)\n",
        "                a_token = a.lower().strip()\n",
        "                samples.append((story_tokens, q_tokens, a_token))\n",
        "            else:\n",
        "                story.append(rest)\n",
        "    return samples\n",
        "\n",
        "TASK_ID = 1\n",
        "LANG = \"en\"\n",
        "train_file = extract_dir / f\"tasks_1-20_v1-2/{LANG}/qa{TASK_ID}_single-supporting-fact_train.txt\"\n",
        "test_file  = extract_dir / f\"tasks_1-20_v1-2/{LANG}/qa{TASK_ID}_single-supporting-fact_test.txt\"\n",
        "\n",
        "train_samples = load_babi_qa(train_file)\n",
        "test_samples  = load_babi_qa(test_file)\n",
        "\n",
        "print(\"Train:\", len(train_samples), \"Test:\", len(test_samples))\n",
        "print(\"Example answer:\", train_samples[0][2])\n",
        "\n",
        "# -------------------------\n",
        "# Vocabulary + encoding\n",
        "# -------------------------\n",
        "SPECIALS = [\"<pad>\", \"<unk>\", \"<sep>\", \"<ssep>\"]\n",
        "PAD, UNK, SEP, SSEP = SPECIALS\n",
        "\n",
        "def build_vocab(samples, min_freq=1):\n",
        "    counter = Counter()\n",
        "    answers = Counter()\n",
        "    for story_tokens, q_tokens, a in samples:\n",
        "        counter.update(story_tokens)\n",
        "        counter.update(q_tokens)\n",
        "        answers[a] += 1\n",
        "    itos = list(SPECIALS)\n",
        "    for tok, c in counter.items():\n",
        "        if c >= min_freq and tok not in itos:\n",
        "            itos.append(tok)\n",
        "    stoi = {t: i for i, t in enumerate(itos)}\n",
        "    ans_itos = sorted(list(answers.keys()))\n",
        "    ans_stoi = {a: i for i, a in enumerate(ans_itos)}\n",
        "    return stoi, itos, ans_stoi, ans_itos\n",
        "\n",
        "stoi, itos, ans_stoi, ans_itos = build_vocab(train_samples + test_samples)\n",
        "vocab_size = len(itos)\n",
        "num_answers = len(ans_itos)\n",
        "print(\"Vocab:\", vocab_size, \"Answers:\", num_answers)\n",
        "\n",
        "def encode_tokens(tokens):\n",
        "    return [stoi.get(t, stoi[UNK]) for t in tokens]\n",
        "\n",
        "MAX_LEN = 180\n",
        "def make_input(story_tokens, q_tokens):\n",
        "    tokens = story_tokens + [SEP] + q_tokens\n",
        "    ids = encode_tokens(tokens)\n",
        "    if len(ids) > MAX_LEN:\n",
        "        ids = ids[-MAX_LEN:]\n",
        "    return ids\n",
        "\n",
        "class BabiQADataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        story_tokens, q_tokens, ans = self.samples[idx]\n",
        "        x = make_input(story_tokens, q_tokens)\n",
        "        y = ans_stoi[ans]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    maxlen = max(x.size(0) for x in xs)\n",
        "    x_pad = torch.full((len(xs), maxlen), fill_value=stoi[PAD], dtype=torch.long)\n",
        "    pad_mask = torch.zeros((len(xs), maxlen), dtype=torch.bool)  # True where PAD\n",
        "    for i, x in enumerate(xs):\n",
        "        x_pad[i, :x.size(0)] = x\n",
        "        pad_mask[i, x.size(0):] = True\n",
        "    y = torch.stack(ys)\n",
        "    return x_pad, pad_mask, y\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(BabiQADataset(train_samples), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
        "test_loader  = DataLoader(BabiQADataset(test_samples),  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# -------------------------\n",
        "# Vanilla Transformer Encoder (no head exposure, no constraints)\n",
        "# -------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1,max_len,D)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=False)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        # x: (T,B,D)\n",
        "        h = self.ln1(x)\n",
        "        attn_out, _ = self.attn(h, h, h, key_padding_mask=key_padding_mask, need_weights=False)\n",
        "        x = x + self.drop(attn_out)\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class VanillaBabiTransformerQA(nn.Module):\n",
        "    def __init__(self, vocab_size, num_answers, d_model=128, n_heads=8, n_layers=2, d_ff=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model, max_len=512)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.layers = nn.ModuleList([EncoderBlock(d_model, n_heads, d_ff, dropout=dropout) for _ in range(n_layers)])\n",
        "        self.ln_final = nn.LayerNorm(d_model)\n",
        "        self.cls = nn.Linear(d_model, num_answers)\n",
        "\n",
        "    def forward(self, x, key_padding_mask):\n",
        "        # x: (B,T), key_padding_mask: (B,T) True where PAD\n",
        "        h = self.embed(x) * math.sqrt(self.d_model)  # (B,T,D)\n",
        "        h = self.pos(h)\n",
        "        h = self.drop(h)\n",
        "        h = h.transpose(0, 1).contiguous()  # (T,B,D)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, key_padding_mask=key_padding_mask)\n",
        "\n",
        "        h = self.ln_final(h)               # (T,B,D)\n",
        "        h_bt = h.transpose(0, 1)           # (B,T,D)\n",
        "\n",
        "        not_pad = (~key_padding_mask).float().unsqueeze(-1)  # (B,T,1)\n",
        "        pooled = (h_bt * not_pad).sum(dim=1) / not_pad.sum(dim=1).clamp(min=1.0)  # (B,D)\n",
        "        logits = self.cls(pooled)\n",
        "        return logits\n",
        "\n",
        "# -------------------------\n",
        "# Train / eval\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x, pad_mask, y in loader:\n",
        "        x, pad_mask, y = x.to(device), pad_mask.to(device), y.to(device)\n",
        "        logits = model(x, pad_mask)\n",
        "        pred = logits.argmax(dim=-1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "def train(model, train_loader, test_loader, epochs=8, lr=3e-4):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for x, pad_mask, y in train_loader:\n",
        "            x, pad_mask, y = x.to(device), pad_mask.to(device), y.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model(x, pad_mask)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            total_loss += float(loss.detach().cpu())\n",
        "        if ep == 1 or ep == epochs:\n",
        "            acc = evaluate(model, test_loader)\n",
        "            print(f\"Epoch {ep:02d}/{epochs} | avg_loss={total_loss/len(train_loader):.4f} | test_acc={acc:.3f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Run\n",
        "# -------------------------\n",
        "model = VanillaBabiTransformerQA(vocab_size=vocab_size, num_answers=num_answers,\n",
        "                                d_model=128, n_heads=8, n_layers=2, d_ff=256, dropout=0.1).to(device)\n",
        "\n",
        "train(model, train_loader, test_loader, epochs=8, lr=3e-4)\n",
        "print(\"Final test accuracy:\", evaluate(model, test_loader))"
      ]
    }
  ]
}