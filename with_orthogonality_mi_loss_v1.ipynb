{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XGKnML4CVhM",
        "outputId": "6b77a024-0bb0-4c8d-c2ed-84d1b1303989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Downloading bAbI tarball...\n",
            "Extracting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3302408973.py:45: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(DATA_ROOT)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 1000 Test samples: 1000\n",
            "Example: (['mary', 'moved', 'to', 'the', 'bathroom', '.', '<ssep>', 'john', 'went', 'to', 'the', 'hallway', '.', '<ssep>'], ['where', 'is', 'mary', '?']) answer: bathroom\n",
            "Vocab size: 25 Num answers: 6\n",
            "Epoch 01 | loss 2.0082 | train_acc 0.244 | test_acc 0.218\n",
            "Epoch 02 | loss 1.9787 | train_acc 0.327 | test_acc 0.290\n",
            "Epoch 03 | loss 1.9503 | train_acc 0.370 | test_acc 0.339\n",
            "Epoch 04 | loss 1.8776 | train_acc 0.375 | test_acc 0.359\n",
            "Epoch 05 | loss 1.7294 | train_acc 0.400 | test_acc 0.363\n",
            "Epoch 06 | loss 1.5675 | train_acc 0.426 | test_acc 0.395\n",
            "Epoch 07 | loss 1.4549 | train_acc 0.442 | test_acc 0.396\n",
            "Epoch 08 | loss 1.3866 | train_acc 0.481 | test_acc 0.409\n",
            "Done.\n",
            "Final test accuracy: 0.409\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Colab-ready bAbI + Transformer + (Orthogonality + MI-difference regularizers)\n",
        "# =========================\n",
        "\n",
        "# If running in Colab, uncomment:\n",
        "# !pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "import os, re, tarfile, random, math\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import urllib.request\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -------------------------\n",
        "# Download bAbI\n",
        "# -------------------------\n",
        "DATA_ROOT = Path(\"./data_babi\")\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "url = \"https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\"\n",
        "tgz_path = DATA_ROOT / \"babi_tasks_1-20_v1-2.tar.gz\"\n",
        "extract_dir = DATA_ROOT\n",
        "\n",
        "if not tgz_path.exists():\n",
        "    print(\"Downloading bAbI tarball...\")\n",
        "    urllib.request.urlretrieve(url, tgz_path)\n",
        "\n",
        "print(\"Extracting...\")\n",
        "with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
        "    tar.extractall(DATA_ROOT)\n",
        "\n",
        "# -------------------------\n",
        "# bAbI parser\n",
        "# -------------------------\n",
        "# bAbI format:\n",
        "# \"1 Mary moved to the bathroom.\"\n",
        "# ...\n",
        "# \"3 Where is Mary?\\tbathroom\\t1\"\n",
        "#\n",
        "# We'll create samples: (story_sentences, question, answer)\n",
        "# and convert to \"context tokens + [SEP] + question tokens\".\n",
        "\n",
        "def tokenize(text: str):\n",
        "    # Keep simple; split on non-letters/numbers, keep punctuation as separate tokens.\n",
        "    # bAbI is small; this works well.\n",
        "    text = text.lower()\n",
        "    # separate punctuation\n",
        "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
        "    tokens = [t for t in text.split() if t.strip()]\n",
        "    return tokens\n",
        "\n",
        "def load_babi_qa(path: Path, max_story_sentences=50):\n",
        "    samples = []\n",
        "    story = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            # line starts with an index\n",
        "            idx_str, rest = line.split(\" \", 1)\n",
        "            idx = int(idx_str)\n",
        "            if idx == 1:\n",
        "                story = []  # new story\n",
        "            if \"\\t\" in rest:\n",
        "                # question line\n",
        "                q, a, supporting = rest.split(\"\\t\")\n",
        "                story_tokens = []\n",
        "                # flatten story sentences into one token list with sentence separators\n",
        "                # to keep it simple, we just concatenate with a special token <ssep>\n",
        "                for sent in story[-max_story_sentences:]:\n",
        "                    story_tokens += tokenize(sent) + [\"<ssep>\"]\n",
        "                q_tokens = tokenize(q)\n",
        "                a_token = a.lower().strip()\n",
        "                samples.append((story_tokens, q_tokens, a_token))\n",
        "            else:\n",
        "                # statement line\n",
        "                story.append(rest)\n",
        "    return samples\n",
        "\n",
        "# Choose a task for speed. Task 1 is commonly used.\n",
        "TASK_ID = 1\n",
        "LANG = \"en\"\n",
        "train_file = extract_dir / f\"tasks_1-20_v1-2/{LANG}/qa{TASK_ID}_single-supporting-fact_train.txt\"\n",
        "test_file  = extract_dir / f\"tasks_1-20_v1-2/{LANG}/qa{TASK_ID}_single-supporting-fact_test.txt\"\n",
        "\n",
        "train_samples = load_babi_qa(train_file)\n",
        "test_samples = load_babi_qa(test_file)\n",
        "\n",
        "print(\"Train samples:\", len(train_samples), \"Test samples:\", len(test_samples))\n",
        "print(\"Example:\", train_samples[0][:2], \"answer:\", train_samples[0][2])\n",
        "\n",
        "# -------------------------\n",
        "# Vocabulary + encoding\n",
        "# -------------------------\n",
        "SPECIALS = [\"<pad>\", \"<unk>\", \"<sep>\", \"<ssep>\"]\n",
        "PAD, UNK, SEP, SSEP = SPECIALS\n",
        "\n",
        "def build_vocab(samples, min_freq=1):\n",
        "    counter = Counter()\n",
        "    answers = Counter()\n",
        "    for story_tokens, q_tokens, a in samples:\n",
        "        counter.update(story_tokens)\n",
        "        counter.update(q_tokens)\n",
        "        answers[a] += 1\n",
        "    itos = list(SPECIALS)\n",
        "    for tok, c in counter.items():\n",
        "        if c >= min_freq and tok not in itos:\n",
        "            itos.append(tok)\n",
        "    stoi = {t:i for i,t in enumerate(itos)}\n",
        "    ans_itos = sorted(list(answers.keys()))\n",
        "    ans_stoi = {a:i for i,a in enumerate(ans_itos)}\n",
        "    return stoi, itos, ans_stoi, ans_itos\n",
        "\n",
        "stoi, itos, ans_stoi, ans_itos = build_vocab(train_samples + test_samples, min_freq=1)\n",
        "vocab_size = len(itos)\n",
        "num_answers = len(ans_itos)\n",
        "print(\"Vocab size:\", vocab_size, \"Num answers:\", num_answers)\n",
        "\n",
        "def encode_tokens(tokens, stoi):\n",
        "    return [stoi.get(t, stoi[UNK]) for t in tokens]\n",
        "\n",
        "# Pack input as: story + [SEP] + question\n",
        "MAX_LEN = 180  # enough for task 1; increase if you switch tasks\n",
        "def make_input(story_tokens, q_tokens):\n",
        "    tokens = story_tokens + [SEP] + q_tokens\n",
        "    ids = encode_tokens(tokens, stoi)\n",
        "    if len(ids) > MAX_LEN:\n",
        "        ids = ids[-MAX_LEN:]  # keep last tokens (question-related)\n",
        "    return ids\n",
        "\n",
        "class BabiQADataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        story_tokens, q_tokens, ans = self.samples[idx]\n",
        "        x = make_input(story_tokens, q_tokens)\n",
        "        y = ans_stoi[ans]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    maxlen = max(x.size(0) for x in xs)\n",
        "    x_pad = torch.full((len(xs), maxlen), fill_value=stoi[PAD], dtype=torch.long)\n",
        "    attn_mask = torch.zeros((len(xs), maxlen), dtype=torch.bool)  # True for PAD positions\n",
        "    for i, x in enumerate(xs):\n",
        "        x_pad[i, :x.size(0)] = x\n",
        "        attn_mask[i, x.size(0):] = True\n",
        "    y = torch.stack(ys)\n",
        "    return x_pad, attn_mask, y\n",
        "\n",
        "train_ds = BabiQADataset(train_samples)\n",
        "test_ds  = BabiQADataset(test_samples)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# -------------------------\n",
        "# Model: Transformer Encoder with explicit per-head outputs\n",
        "# -------------------------\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, D)\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:, :T]\n",
        "\n",
        "class MultiHeadSelfAttentionExpose(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps nn.MultiheadAttention but returns:\n",
        "    - output: (T, B, D)\n",
        "    - per_head_attn: (B, H, T, T)\n",
        "    - per_head_value_out: (B, H, T, Dh)  (head-specific output before mixing)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "\n",
        "        # We'll implement QKV ourselves to easily expose head outputs.\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "        self.o = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        # x: (T, B, D)\n",
        "        T, B, D = x.shape\n",
        "        qkv = self.qkv(x)  # (T, B, 3D)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)  # each (T, B, D)\n",
        "\n",
        "        # reshape to heads: (B, H, T, Dh)\n",
        "        def to_heads(t):\n",
        "            t = t.permute(1, 0, 2).contiguous()  # (B, T, D)\n",
        "            t = t.view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, T, Dh)\n",
        "            return t\n",
        "\n",
        "        qh = to_heads(q)\n",
        "        kh = to_heads(k)\n",
        "        vh = to_heads(v)\n",
        "\n",
        "        # scaled dot-product attention\n",
        "        scores = torch.matmul(qh, kh.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B,H,T,T)\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # key_padding_mask: (B, T) True where PAD\n",
        "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # (B,1,1,T)\n",
        "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)  # (B,H,T,T)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        head_out = torch.matmul(attn, vh)  # (B,H,T,Dh)\n",
        "        # merge heads\n",
        "        merged = head_out.transpose(1, 2).contiguous().view(B, T, D)  # (B,T,D)\n",
        "        out = self.o(merged)  # (B,T,D)\n",
        "\n",
        "        return out.permute(1, 0, 2).contiguous(), attn, head_out\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = MultiHeadSelfAttentionExpose(d_model, n_heads, dropout=dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        # pre-norm\n",
        "        h = self.ln1(x)\n",
        "        attn_out, attn_w, head_out = self.attn(h, key_padding_mask=key_padding_mask)\n",
        "        x = x + attn_out\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x, attn_w, head_out\n",
        "\n",
        "class BabiTransformerQA(nn.Module):\n",
        "    def __init__(self, vocab_size, num_answers, d_model=128, n_heads=8, n_layers=2, d_ff=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model, max_len=512)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layers = nn.ModuleList([EncoderBlock(d_model, n_heads, d_ff, dropout=dropout) for _ in range(n_layers)])\n",
        "        self.ln_final = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, num_answers)\n",
        "\n",
        "    def forward(self, x, key_padding_mask):\n",
        "        \"\"\"\n",
        "        x: (B, T)\n",
        "        key_padding_mask: (B, T) True where PAD\n",
        "        Returns:\n",
        "          logits: (B, num_answers)\n",
        "          head_reprs: (B, H, D) pooled per-head representations (for MI/ortho losses)\n",
        "        \"\"\"\n",
        "        B, T = x.shape\n",
        "        h = self.embed(x) * math.sqrt(self.d_model)  # (B,T,D)\n",
        "        h = self.pos(h)\n",
        "        h = self.dropout(h)\n",
        "        h = h.transpose(0, 1).contiguous()  # (T,B,D)\n",
        "\n",
        "        all_head_out = []  # list of (B,H,T,Dh) per layer\n",
        "        for layer in self.layers:\n",
        "            h, attn_w, head_out = layer(h, key_padding_mask=key_padding_mask)\n",
        "            all_head_out.append(head_out)\n",
        "\n",
        "        h = self.ln_final(h)  # (T,B,D)\n",
        "        # Pool final token representation (last non-pad) OR simply mean-pool non-pad\n",
        "        h_bt = h.transpose(0,1)  # (B,T,D)\n",
        "\n",
        "        not_pad = (~key_padding_mask).float().unsqueeze(-1)  # (B,T,1)\n",
        "        pooled = (h_bt * not_pad).sum(dim=1) / (not_pad.sum(dim=1).clamp(min=1.0))  # (B,D)\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        # Build per-head representations from the LAST layer head outputs:\n",
        "        # head_out: (B,H,T,Dh). We'll mean-pool over tokens (non-pad), then project to D.\n",
        "        last_head = all_head_out[-1]  # (B,H,T,Dh)\n",
        "        mask_h = (~key_padding_mask).float().unsqueeze(1).unsqueeze(-1)  # (B,1,T,1)\n",
        "        head_pooled = (last_head * mask_h).sum(dim=2) / (mask_h.sum(dim=2).clamp(min=1.0))  # (B,H,Dh)\n",
        "        # Expand to D by simple linear mapping per head (shared)\n",
        "        # Option: just pad to D via a learned projection:\n",
        "        head_proj = F.pad(head_pooled, (0, self.d_model - head_pooled.size(-1)))  # (B,H,D) if Dh < D\n",
        "        return logits, head_proj\n",
        "\n",
        "# -------------------------\n",
        "# Regularizers\n",
        "# -------------------------\n",
        "\n",
        "def orthogonality_loss(head_repr):\n",
        "    \"\"\"\n",
        "    head_repr: (B, H, D)\n",
        "    Encourage different heads to be orthogonal in representation space.\n",
        "    Compute Gram matrix per sample: G = H x H via normalized dot products.\n",
        "    Penalize off-diagonal energy.\n",
        "    \"\"\"\n",
        "    B, H, D = head_repr.shape\n",
        "    z = F.normalize(head_repr, dim=-1)  # (B,H,D)\n",
        "    G = torch.matmul(z, z.transpose(1,2))  # (B,H,H)\n",
        "    I = torch.eye(H, device=head_repr.device).unsqueeze(0)  # (1,H,H)\n",
        "    off_diag = G - I\n",
        "    return (off_diag ** 2).mean()\n",
        "\n",
        "def info_nce_mi_proxy_loss(head_repr, temperature=0.2):\n",
        "    \"\"\"\n",
        "    Approximate mutual information between pairs of heads with an InfoNCE-style objective.\n",
        "    We then MINIMIZE this proxy to \"maximize differences\" (i.e., reduce dependence).\n",
        "\n",
        "    For each pair of heads (i,j):\n",
        "      positives: (h_i[b], h_j[b]) same sample b\n",
        "      negatives: (h_i[b], h_j[b']) with shuffled b'\n",
        "    We compute an InfoNCE loss that would normally maximize alignment/MI;\n",
        "    minimizing it discourages shared info.\n",
        "\n",
        "    head_repr: (B,H,D)\n",
        "    \"\"\"\n",
        "    B, H, D = head_repr.shape\n",
        "    z = F.normalize(head_repr, dim=-1)  # (B,H,D)\n",
        "\n",
        "    total = 0.0\n",
        "    count = 0\n",
        "    for i in range(H):\n",
        "        zi = z[:, i, :]  # (B,D)\n",
        "        for j in range(i+1, H):\n",
        "            zj = z[:, j, :]  # (B,D)\n",
        "            # logits: (B,B) similarity between zi[b] and zj[b']\n",
        "            logits = torch.matmul(zi, zj.t()) / temperature  # (B,B)\n",
        "            labels = torch.arange(B, device=head_repr.device)\n",
        "            # Standard InfoNCE (cross-entropy) would encourage zi[b] close to zj[b]\n",
        "            # We MINIMIZE this to reduce dependence => \"maximize differences\"\n",
        "            total = total + F.cross_entropy(logits, labels)\n",
        "            count += 1\n",
        "    return total / max(count, 1)\n",
        "\n",
        "# -------------------------\n",
        "# Train / Eval\n",
        "# -------------------------\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, pad_mask, y in loader:\n",
        "            x, pad_mask, y = x.to(device), pad_mask.to(device), y.to(device)\n",
        "            logits, _ = model(x, pad_mask)\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "# -------------------------\n",
        "# Hyperparameters (small + fast)\n",
        "# -------------------------\n",
        "d_model = 192\n",
        "n_heads = 16\n",
        "n_layers = 3\n",
        "d_ff = 256\n",
        "dropout = 0.1\n",
        "\n",
        "lambda_ortho = 0.1    # weight for orthogonality constraint\n",
        "lambda_mi = 0.05      # weight for MI-difference constraint (minimize MI proxy)\n",
        "\n",
        "model = BabiTransformerQA(\n",
        "    vocab_size=vocab_size,\n",
        "    num_answers=num_answers,\n",
        "    d_model=d_model,\n",
        "    n_heads=n_heads,\n",
        "    n_layers=n_layers,\n",
        "    d_ff=d_ff,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
        "\n",
        "EPOCHS = 8  # Task 1 converges quickly\n",
        "print_every = 1\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "\n",
        "    for x, pad_mask, y in train_loader:\n",
        "        x, pad_mask, y = x.to(device), pad_mask.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits, head_repr = model(x, pad_mask)\n",
        "\n",
        "        ce = F.cross_entropy(logits, y)\n",
        "        ortho = orthogonality_loss(head_repr)\n",
        "        mi_proxy = info_nce_mi_proxy_loss(head_repr, temperature=0.2)\n",
        "\n",
        "        loss = ce + lambda_ortho * ortho + lambda_mi * mi_proxy\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running += loss.item()\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        train_acc = evaluate(model, train_loader)\n",
        "        test_acc = evaluate(model, test_loader)\n",
        "        avg_loss = running / max(len(train_loader), 1)\n",
        "        print(f\"Epoch {epoch:02d} | loss {avg_loss:.4f} | train_acc {train_acc:.3f} | test_acc {test_acc:.3f}\")\n",
        "\n",
        "print(\"Done.\")\n",
        "print(\"Final test accuracy:\", evaluate(model, test_loader))\n"
      ]
    }
  ]
}